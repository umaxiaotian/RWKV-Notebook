{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umaxiaotian/RWKV-Notebook/blob/main/RWKV_ChatRWKV_WORLD%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA%E5%B0%82%E7%94%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8bZFEPLrlWr"
      },
      "source": [
        "# ChatRWKV-RWKV4-Worldシリーズ専用\n",
        "\n",
        "[RWKV](https://github.com/BlinkDL/RWKV-LM)はtransformerレベルの性能を持つRNNです。\n",
        "\n",
        "このノートブックは [ChatRWKV](https://github.com/BlinkDL/RWKV-LM) RWKVの推論用です。\n",
        "\n",
        "※このノートブックは[umaxiaotian](https://github.com/umaxiaotian)により、RWKV-WorldをGoogleColabで実行できるようにしたものです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HUIQ3-rcw2O"
      },
      "outputs": [],
      "source": [
        "#@title GoogleDrive接続オプション { display-mode: \"form\" }\n",
        "save_models_to_drive = True #@param {type:\"boolean\"}\n",
        "drive_mount = '/content/drive' #@param {type:\"string\"}\n",
        "model_dir = 'rwkv-4-world-model' #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "if save_models_to_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount(drive_mount)\n",
        "    model_dir_path = f\"{drive_mount}/MyDrive/{model_dir}\" if save_models_to_drive else f\"/content/{model_dir}\"\n",
        "else:\n",
        "    model_dir_path = \"/content\"\n",
        "\n",
        "os.makedirs(f\"{model_dir_path}\", exist_ok=True)\n",
        "\n",
        "print(f\"Saving models to {model_dir_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/BlinkDL/ChatRWKV"
      ],
      "metadata": {
        "id": "upx44PXgchfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ninja tokenizers rwkv prompt_toolkit"
      ],
      "metadata": {
        "id": "VozXngWX_JeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-Mrn6tTeEub"
      },
      "outputs": [],
      "source": [
        "#@title モデルの選択とダウンロード { display-mode: \"form\" }\n",
        "import urllib\n",
        "\n",
        "# @markdown ご希望のモデルをお選びください：\n",
        "model_file = \"RWKV-4-World-JPNtuned-7B-v1-20230718-ctx4096.pth\" #@param {type:\"string\"}\n",
        "# @markdown 最初に `model_dir` から `model_file` を検索する。\n",
        "# @markdown  有効なパスでない場合、huggingfaceから `RWKV-4-World` モデルのダウンロードを試みます。\n",
        "# @markdown  どのようなオプションがあるかは[repo](https://huggingface.co/BlinkDL/rwkv-4-world/)を見てください。\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown 例:\n",
        "#@markdown ---\n",
        "#@markdown - RWKV-4-World-JPNtuned-7B: `RWKV-4-World-JPNtuned-7B-v1-20230718-ctx4096.pth`\n",
        "#@markdown - RWKV-4-World-CHNtuned-7B: `RWKV-4-World-CHNtuned-7B-v1-20230709-ctx4096.pth`\n",
        "\n",
        "\n",
        "model_path = f\"{model_dir_path}/{model_file}\"\n",
        "if not os.path.exists(model_path):\n",
        "    model_repo = f\"https://huggingface.co/BlinkDL/rwkv-4-world/resolve/main\"\n",
        "    model_url = f\"{model_repo}/{urllib.parse.quote_plus(model_file)}\"\n",
        "    try:\n",
        "        print(f\"Downloading '{model_file}' from {model_url} this may take a while\")\n",
        "        urllib.request.urlretrieve(model_url, model_path)\n",
        "        print(f\"Using {model_path} as base\")\n",
        "    except Exception as e:\n",
        "        print(f\"Model '{model_file}' doesn't exist\")\n",
        "        raise Exception\n",
        "else:\n",
        "    print(f\"Using {model_path} as base\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title モデルのロード {\"display-mode\": \"form\"}\n",
        "import os, copy, types, gc, sys\n",
        "import numpy as np\n",
        "import torch\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "strategy = 'cuda fp16' #@param {\"type\": \"string\"}\n",
        "\n",
        "\n",
        "RWKV_JIT_ON = '1' #@param {\"type\": \"string\"}\n",
        "RWKV_CUDA_ON = '1' #@param {\"type\": \"string\"}\n",
        "os.environ[\"RWKV_JIT_ON\"] = '1' # '1' or '0', please use torch 1.13+ and benchmark speed\n",
        "os.environ[\"RWKV_CUDA_ON\"] = '1' # '1' to compile CUDA kernel (10x faster), requires c++ compiler & cuda libraries\n",
        "\n",
        "\n",
        "\n",
        "#@markdown Strategyの例:\n",
        "#@markdown - `cpu fp32`\n",
        "#@markdown - `cuda:0 fp16 -> cuda:1 fp16`\n",
        "#@markdown - `cuda fp16i8 *10 -> cuda fp16`\n",
        "#@markdown - `cuda fp16i8`\n",
        "#@markdown - `cuda fp16i8 -> cpu fp32 *10`\n",
        "#@markdown - `cuda fp16i8 *10+`\n",
        "\n",
        "#@markdown RWKV_JIT_ONの例:\n",
        "#@markdown - `1` でJITコンパイラ有効　（基本有効で）\n",
        "#@markdown - `0` でJITコンパイラ無効\n",
        "\n",
        "#@markdown RWKV_CUDA_ONの例:\n",
        "#@markdown - `1` でCUDA有効　(GPUを使用する際に有効です。)\n",
        "#@markdown - `0` でCUDA無効\n"
      ],
      "metadata": {
        "id": "WxMnXXJYI_RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title モデルのセットアップ {\"display-mode\": \"form\"}\n",
        "\n",
        "\n",
        "CHAT_LANG = 'Japanese'  #@param {\"type\": \"string\"}\n",
        "\n",
        "\n",
        "# -1.py for [User & Bot] (Q&A) prompt\n",
        "# -2.py for [Bob & Alice] (chat) prompt\n",
        "PROMPT_FILE = f'/content/ChatRWKV/v2/prompt/default/{CHAT_LANG}-2.py'\n",
        "\n",
        "CHAT_LEN_SHORT = 40\n",
        "CHAT_LEN_LONG = 150\n",
        "FREE_GEN_LEN = 256\n",
        "\n",
        "# For better chat & QA quality: reduce temp, reduce top-p, increase repetition penalties\n",
        "# Explanation: https://platform.openai.com/docs/api-reference/parameter-details\n",
        "GEN_TEMP = 1.2  #@param {\"type\": \"string\"} # It could be a good idea to increase temp when top_p is low\n",
        "GEN_TOP_P = 0.5  #@param {\"type\": \"string\"} # Reduce top_p (to 0.5, 0.2, 0.1 etc.) for better Q&A accuracy (and less diversity)\n",
        "GEN_alpha_presence = 0.4  #@param {\"type\": \"string\"} # Presence Penalty\n",
        "GEN_alpha_frequency = 0.4  #@param {\"type\": \"string\"} # Frequency Penalty\n",
        "GEN_penalty_decay = 0.996  #@param {\"type\": \"string\"}\n",
        "AVOID_REPEAT = '，：？！'\n",
        "\n",
        "CHUNK_LEN = 256 # split input into chunks to save VRAM (shorter -> slower)\n",
        "########################################################################################################\n",
        "\n",
        "print(f'\\n{CHAT_LANG} - {strategy} - {PROMPT_FILE}')\n",
        "from rwkv.model import RWKV\n",
        "from rwkv.utils import PIPELINE\n",
        "\n",
        "def load_prompt(PROMPT_FILE):\n",
        "    variables = {}\n",
        "    with open(PROMPT_FILE, 'rb') as file:\n",
        "        exec(compile(file.read(), PROMPT_FILE, 'exec'), variables)\n",
        "    user, bot, interface, init_prompt = variables['user'], variables['bot'], variables['interface'], variables['init_prompt']\n",
        "    init_prompt = init_prompt.strip().split('\\n')\n",
        "    for c in range(len(init_prompt)):\n",
        "        init_prompt[c] = init_prompt[c].strip().strip('\\u3000').strip('\\r')\n",
        "    init_prompt = '\\n' + ('\\n'.join(init_prompt)).strip() + '\\n\\n'\n",
        "    return user, bot, interface, init_prompt\n",
        "\n",
        "# Load Model\n",
        "\n",
        "print(f'Loading model - {model_path}')\n",
        "model = RWKV(model=model_path, strategy=strategy)\n",
        "\n",
        "pipeline = PIPELINE(model, \"rwkv_vocab_v20230424\")\n",
        "END_OF_TEXT = 0\n",
        "END_OF_LINE = 11\n",
        "\n",
        "# pipeline = PIPELINE(model, \"cl100k_base\")\n",
        "# END_OF_TEXT = 100257\n",
        "# END_OF_LINE = 198\n",
        "\n",
        "model_tokens = []\n",
        "model_state = None\n",
        "\n",
        "AVOID_REPEAT_TOKENS = []\n",
        "for i in AVOID_REPEAT:\n",
        "    dd = pipeline.encode(i)\n",
        "    assert len(dd) == 1\n",
        "    AVOID_REPEAT_TOKENS += dd\n",
        "\n",
        "########################################################################################################\n",
        "\n",
        "def run_rnn(tokens, newline_adj = 0):\n",
        "    global model_tokens, model_state\n",
        "\n",
        "    tokens = [int(x) for x in tokens]\n",
        "    model_tokens += tokens\n",
        "    # print(f'### model ###\\n{tokens}\\n[{pipeline.decode(model_tokens)}]')\n",
        "\n",
        "    while len(tokens) > 0:\n",
        "        out, model_state = model.forward(tokens[:CHUNK_LEN], model_state)\n",
        "        tokens = tokens[CHUNK_LEN:]\n",
        "\n",
        "    out[END_OF_LINE] += newline_adj # adjust \\n probability\n",
        "\n",
        "    if model_tokens[-1] in AVOID_REPEAT_TOKENS:\n",
        "        out[model_tokens[-1]] = -999999999\n",
        "    return out\n",
        "\n",
        "all_state = {}\n",
        "def save_all_stat(srv, name, last_out):\n",
        "    n = f'{name}_{srv}'\n",
        "    all_state[n] = {}\n",
        "    all_state[n]['out'] = last_out\n",
        "    all_state[n]['rnn'] = copy.deepcopy(model_state)\n",
        "    all_state[n]['token'] = copy.deepcopy(model_tokens)\n",
        "\n",
        "def load_all_stat(srv, name):\n",
        "    global model_tokens, model_state\n",
        "    n = f'{name}_{srv}'\n",
        "    model_state = copy.deepcopy(all_state[n]['rnn'])\n",
        "    model_tokens = copy.deepcopy(all_state[n]['token'])\n",
        "    return all_state[n]['out']\n",
        "\n",
        "# Model only saw '\\n\\n' as [187, 187] before, but the tokenizer outputs [535] for it at the end\n",
        "def fix_tokens(tokens):\n",
        "        return tokens\n",
        "#@markdown パラメータ説明\n",
        "#@markdown - GEN_TEMP : 温度\n",
        "#@markdown - GEN_TOP_P : top-P\n",
        "#@markdown - GEN_alpha_presence : 同じトークンの繰り返しを減らすため出力トークンに課すペナルティ (-2〜2)\n",
        "#@markdown - GEN_penalty_decay : 同じテキスト出力をへらすため出力トークンに課すペナルティ (-2〜2)\n",
        "#@markdown - GEN_penalty_decay : 新しいトークンがこれまでのテキストに表示されているかどうかに基づいてペナルティを課し、モデルが新しいトピックについて話す可能性を高めます。\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown [API](https://platform.openai.com/docs/api-reference/completions/create)に基づいて設定されています。"
      ],
      "metadata": {
        "id": "L1wBpmJjQXSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0lOYL6PFvpN"
      },
      "outputs": [],
      "source": [
        "#@title デモの出力 {\"display-mode\": \"form\"}\n",
        "\n",
        "# Run inference\n",
        "print(f'\\nRun prompt...')\n",
        "\n",
        "user, bot, interface, init_prompt = load_prompt(PROMPT_FILE)\n",
        "out = run_rnn(fix_tokens(pipeline.encode(init_prompt)))\n",
        "save_all_stat('', 'chat_init', out)\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "srv_list = ['dummy_server']\n",
        "for s in srv_list:\n",
        "    save_all_stat(s, 'chat', out)\n",
        "\n",
        "def reply_msg(msg):\n",
        "    print(f'{bot}{interface} {msg}\\n')\n",
        "\n",
        "def on_message(message):\n",
        "    global model_tokens, model_state, user, bot, interface, init_prompt\n",
        "\n",
        "    srv = 'dummy_server'\n",
        "\n",
        "    msg = message.replace('\\\\n','\\n').strip()\n",
        "\n",
        "    x_temp = GEN_TEMP\n",
        "    x_top_p = GEN_TOP_P\n",
        "    if (\"-temp=\" in msg):\n",
        "        x_temp = float(msg.split(\"-temp=\")[1].split(\" \")[0])\n",
        "        msg = msg.replace(\"-temp=\"+f'{x_temp:g}', \"\")\n",
        "        # print(f\"temp: {x_temp}\")\n",
        "    if (\"-top_p=\" in msg):\n",
        "        x_top_p = float(msg.split(\"-top_p=\")[1].split(\" \")[0])\n",
        "        msg = msg.replace(\"-top_p=\"+f'{x_top_p:g}', \"\")\n",
        "        # print(f\"top_p: {x_top_p}\")\n",
        "    if x_temp <= 0.2:\n",
        "        x_temp = 0.2\n",
        "    if x_temp >= 5:\n",
        "        x_temp = 5\n",
        "    if x_top_p <= 0:\n",
        "        x_top_p = 0\n",
        "    msg = msg.strip()\n",
        "\n",
        "    if msg == '+reset':\n",
        "        out = load_all_stat('', 'chat_init')\n",
        "        save_all_stat(srv, 'chat', out)\n",
        "        reply_msg(\"Chat reset.\")\n",
        "        return\n",
        "\n",
        "    # use '+prompt {path}' to load a new prompt\n",
        "    elif msg[:8].lower() == '+prompt ':\n",
        "        print(\"Loading prompt...\")\n",
        "        try:\n",
        "            PROMPT_FILE = msg[8:].strip()\n",
        "            user, bot, interface, init_prompt = load_prompt(PROMPT_FILE)\n",
        "            out = run_rnn(fix_tokens(pipeline.encode(init_prompt)))\n",
        "            save_all_stat(srv, 'chat', out)\n",
        "            print(\"Prompt set up.\")\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "        except:\n",
        "            print(\"Path error.\")\n",
        "\n",
        "    elif msg[:5].lower() == '+gen ' or msg[:3].lower() == '+i ' or msg[:4].lower() == '+qa ' or msg[:4].lower() == '+qq ' or msg.lower() == '+++' or msg.lower() == '++':\n",
        "\n",
        "        if msg[:5].lower() == '+gen ':\n",
        "            new = '\\n' + msg[5:].strip()\n",
        "            # print(f'### prompt ###\\n[{new}]')\n",
        "            model_state = None\n",
        "            model_tokens = []\n",
        "            out = run_rnn(pipeline.encode(new))\n",
        "            save_all_stat(srv, 'gen_0', out)\n",
        "\n",
        "        elif msg[:3].lower() == '+i ':\n",
        "            msg = msg[3:].strip().replace('\\r\\n','\\n').replace('\\n\\n','\\n')\n",
        "            new = f'''\n",
        "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "# Instruction:\n",
        "{msg}\n",
        "\n",
        "# Response:\n",
        "'''\n",
        "            # print(f'### prompt ###\\n[{new}]')\n",
        "            model_state = None\n",
        "            model_tokens = []\n",
        "            out = run_rnn(pipeline.encode(new))\n",
        "            save_all_stat(srv, 'gen_0', out)\n",
        "\n",
        "        elif msg[:4].lower() == '+qq ':\n",
        "            new = '\\nQ: ' + msg[4:].strip() + '\\nA:'\n",
        "            # print(f'### prompt ###\\n[{new}]')\n",
        "            model_state = None\n",
        "            model_tokens = []\n",
        "            out = run_rnn(pipeline.encode(new))\n",
        "            save_all_stat(srv, 'gen_0', out)\n",
        "\n",
        "        elif msg[:4].lower() == '+qa ':\n",
        "            out = load_all_stat('', 'chat_init')\n",
        "\n",
        "            real_msg = msg[4:].strip()\n",
        "            new = f\"{user}{interface} {real_msg}\\n\\n{bot}{interface}\"\n",
        "            # print(f'### qa ###\\n[{new}]')\n",
        "\n",
        "            out = run_rnn(pipeline.encode(new))\n",
        "            save_all_stat(srv, 'gen_0', out)\n",
        "\n",
        "        elif msg.lower() == '+++':\n",
        "            try:\n",
        "                out = load_all_stat(srv, 'gen_1')\n",
        "                save_all_stat(srv, 'gen_0', out)\n",
        "            except:\n",
        "                return\n",
        "\n",
        "        elif msg.lower() == '++':\n",
        "            try:\n",
        "                out = load_all_stat(srv, 'gen_0')\n",
        "            except:\n",
        "                return\n",
        "\n",
        "        begin = len(model_tokens)\n",
        "        out_last = begin\n",
        "        occurrence = {}\n",
        "        for i in range(FREE_GEN_LEN+100):\n",
        "            for n in occurrence:\n",
        "                out[n] -= (GEN_alpha_presence + occurrence[n] * GEN_alpha_frequency)\n",
        "            token = pipeline.sample_logits(\n",
        "                out,\n",
        "                temperature=x_temp,\n",
        "                top_p=x_top_p,\n",
        "            )\n",
        "            if token == END_OF_TEXT:\n",
        "                break\n",
        "            for xxx in occurrence:\n",
        "                occurrence[xxx] *= GEN_penalty_decay\n",
        "            if token not in occurrence:\n",
        "                occurrence[token] = 1\n",
        "            else:\n",
        "                occurrence[token] += 1\n",
        "\n",
        "            if msg[:4].lower() == '+qa ':# or msg[:4].lower() == '+qq ':\n",
        "                out = run_rnn([token], newline_adj=-2)\n",
        "            else:\n",
        "                out = run_rnn([token])\n",
        "\n",
        "            xxx = pipeline.decode(model_tokens[out_last:])\n",
        "            if '\\ufffd' not in xxx: # avoid utf-8 display issues\n",
        "                print(xxx, end='', flush=True)\n",
        "                out_last = begin + i + 1\n",
        "                if i >= FREE_GEN_LEN:\n",
        "                    break\n",
        "        print('\\n')\n",
        "        # send_msg = pipeline.decode(model_tokens[begin:]).strip()\n",
        "        # print(f'### send ###\\n[{send_msg}]')\n",
        "        # reply_msg(send_msg)\n",
        "        save_all_stat(srv, 'gen_1', out)\n",
        "\n",
        "    else:\n",
        "        if msg.lower() == '+':\n",
        "            try:\n",
        "                out = load_all_stat(srv, 'chat_pre')\n",
        "            except:\n",
        "                return\n",
        "        else:\n",
        "            out = load_all_stat(srv, 'chat')\n",
        "            msg = msg.strip().replace('\\r\\n','\\n').replace('\\n\\n','\\n')\n",
        "            new = f\"{user}{interface} {msg}\\n\\n{bot}{interface}\"\n",
        "            # print(f'### add ###\\n[{new}]')\n",
        "            out = run_rnn(pipeline.encode(new), newline_adj=-999999999)\n",
        "            save_all_stat(srv, 'chat_pre', out)\n",
        "\n",
        "        begin = len(model_tokens)\n",
        "        out_last = begin\n",
        "        print(f'{bot}{interface}', end='', flush=True)\n",
        "        occurrence = {}\n",
        "        for i in range(999):\n",
        "            if i <= 0:\n",
        "                newline_adj = -999999999\n",
        "            elif i <= CHAT_LEN_SHORT:\n",
        "                newline_adj = (i - CHAT_LEN_SHORT) / 10\n",
        "            elif i <= CHAT_LEN_LONG:\n",
        "                newline_adj = 0\n",
        "            else:\n",
        "                newline_adj = min(3, (i - CHAT_LEN_LONG) * 0.25) # MUST END THE GENERATION\n",
        "\n",
        "            for n in occurrence:\n",
        "                out[n] -= (GEN_alpha_presence + occurrence[n] * GEN_alpha_frequency)\n",
        "            token = pipeline.sample_logits(\n",
        "                out,\n",
        "                temperature=x_temp,\n",
        "                top_p=x_top_p,\n",
        "            )\n",
        "            # if token == END_OF_TEXT:\n",
        "            #     break\n",
        "            for xxx in occurrence:\n",
        "                occurrence[xxx] *= GEN_penalty_decay\n",
        "            if token not in occurrence:\n",
        "                occurrence[token] = 1\n",
        "            else:\n",
        "                occurrence[token] += 1\n",
        "\n",
        "            out = run_rnn([token], newline_adj=newline_adj)\n",
        "            out[END_OF_TEXT] = -999999999  # disable <|endoftext|>\n",
        "\n",
        "            xxx = pipeline.decode(model_tokens[out_last:])\n",
        "            if '\\ufffd' not in xxx: # avoid utf-8 display issues\n",
        "                print(xxx, end='', flush=True)\n",
        "                out_last = begin + i + 1\n",
        "\n",
        "            send_msg = pipeline.decode(model_tokens[begin:])\n",
        "            if '\\n\\n' in send_msg:\n",
        "                send_msg = send_msg.strip()\n",
        "                break\n",
        "\n",
        "            # send_msg = pipeline.decode(model_tokens[begin:]).strip()\n",
        "            # if send_msg.endswith(f'{user}{interface}'): # warning: needs to fix state too !!!\n",
        "            #     send_msg = send_msg[:-len(f'{user}{interface}')].strip()\n",
        "            #     break\n",
        "            # if send_msg.endswith(f'{bot}{interface}'):\n",
        "            #     send_msg = send_msg[:-len(f'{bot}{interface}')].strip()\n",
        "            #     break\n",
        "\n",
        "        # print(f'{model_tokens}')\n",
        "        # print(f'[{pipeline.decode(model_tokens)}]')\n",
        "\n",
        "        # print(f'### send ###\\n[{send_msg}]')\n",
        "        # reply_msg(send_msg)\n",
        "        save_all_stat(srv, 'chat', out)\n",
        "\n",
        "########################################################################################################\n",
        "\n",
        "if CHAT_LANG == 'English':\n",
        "    HELP_MSG = '''Commands:\n",
        "say something --> chat with bot. use \\\\n for new line.\n",
        "+ --> alternate chat reply\n",
        "+reset --> reset chat\n",
        "\n",
        "+gen YOUR PROMPT --> free single-round generation with any prompt. use \\\\n for new line.\n",
        "+i YOUR INSTRUCT --> free single-round generation with any instruct. use \\\\n for new line.\n",
        "+++ --> continue last free generation (only for +gen / +i)\n",
        "++ --> retry last free generation (only for +gen / +i)\n",
        "\n",
        "Now talk with the bot and enjoy. Remember to +reset periodically to clean up the bot's memory. Use RWKV-4 14B (especially https://huggingface.co/BlinkDL/rwkv-4-raven) for best results.\n",
        "'''\n",
        "elif CHAT_LANG == 'Chinese':\n",
        "    HELP_MSG = f'''指令:\n",
        "直接输入内容 --> 和机器人聊天（建议问机器人问题），用\\\\n代表换行，必须用 Raven 模型\n",
        "+ --> 让机器人换个回答\n",
        "+reset --> 重置对话，请经常使用 +reset 重置机器人记忆\n",
        "\n",
        "+i 某某指令 --> 问独立的问题（忽略聊天上下文），用\\\\n代表换行，必须用 Raven 模型\n",
        "+gen 某某内容 --> 续写内容（忽略聊天上下文），用\\\\n代表换行，写小说用 testNovel 模型\n",
        "+++ --> 继续 +gen / +i 的回答\n",
        "++ --> 换个 +gen / +i 的回答\n",
        "\n",
        "作者：彭博 请关注我的知乎: https://zhuanlan.zhihu.com/p/603840957\n",
        "如果喜欢，请看我们的优质护眼灯: https://withablink.taobao.com\n",
        "\n",
        "中文 Novel 模型，可以试这些续写例子（不适合 Raven 模型）：\n",
        "+gen “区区\n",
        "+gen 以下是不朽的科幻史诗长篇巨著，描写细腻，刻画了数百位个性鲜明的英雄和宏大的星际文明战争。\\\\n第一章\n",
        "+gen 这是一个修真世界，详细世界设定如下：\\\\n1.\n",
        "'''\n",
        "elif CHAT_LANG == 'Japanese':\n",
        "    HELP_MSG = f'''コマンド:\n",
        "直接入力 --> ボットとチャットする．改行には\\\\nを使用してください．\n",
        "+ --> ボットに前回のチャットの内容を変更させる．\n",
        "+reset --> 対話のリセット．メモリをリセットするために，+resetを定期的に実行してください．\n",
        "\n",
        "+i インストラクトの入力 --> チャットの文脈を無視して独立した質問を行う．改行には\\\\nを使用してください．\n",
        "+gen プロンプトの生成 --> チャットの文脈を無視して入力したプロンプトに続く文章を出力する．改行には\\\\nを使用してください．\n",
        "+++ --> +gen / +i の出力の回答を続ける．\n",
        "++ --> +gen / +i の出力の再生成を行う.\n",
        "\n",
        "ボットとの会話を楽しんでください。また、定期的に+resetして、ボットのメモリをリセットすることを忘れないようにしてください。\n",
        "'''\n",
        "\n",
        "print(f'{pipeline.decode(model_tokens)}'.replace(f'\\n\\n{bot}',f'\\n{bot}'), end='')\n",
        "\n",
        "########################################################################################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title チャット {\"display-mode\": \"form\"}\n",
        "\n",
        "#@markdown このセルを実行するとチャットが開始されます。入力欄にメッセージを簡単に入力してください。\n",
        "\n",
        "#@markdown コマンド:\n",
        "#@markdown - `+`：代替のチャット応答を取得するために使用します\n",
        "#@markdown - `+reset`：チャットをリセットするために使用します\n",
        "#@markdown - `+gen YOUR PROMPT`：任意のプロンプトでの無料の単一ラウンド生成に使用します\n",
        "#@markdown - `+i YOUR INSTRUCT`：任意の指示での無料の単一ラウンド生成に使用します\n",
        "#@markdown - `+++`：最後の無料生成を続行するために使用します（`+gen` / `+i`のみ）\n",
        "#@markdown - `++`：最後の無料生成を再試行するために使用します（`+gen` / `+i`のみ）\n",
        "\n",
        "#@markdown 定期的にボットのメモリをクリーンアップするために、`+reset`を実行することをお忘れなく。\n",
        "\n",
        "from prompt_toolkit import prompt\n",
        "while True:\n",
        "    msg = input(\"Bob: \")\n",
        "    if len(msg.strip()) > 0:\n",
        "        on_message(msg)\n",
        "    else:\n",
        "        print('Error: please say something')"
      ],
      "metadata": {
        "id": "AtdWuOEQN0S4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}